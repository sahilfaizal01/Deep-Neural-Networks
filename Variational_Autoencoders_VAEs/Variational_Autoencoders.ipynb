{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZeszxhWpXUr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import save_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "18LbdRIwpZqi"
      },
      "outputs": [],
      "source": [
        "# Define hyperparams\n",
        "image_size = 784 # 28 x 28\n",
        "hidden_dim = 400\n",
        "latent_dim = 20\n",
        "batch_size = 128\n",
        "epochs = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cScE7jxSpZtC"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMegfxB0pZvK",
        "outputId": "6744ed2a-71d2-4cb9-d640-9f7acf6d21cc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 16.1MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 479kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.46MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 7.95MB/s]\n"
          ]
        }
      ],
      "source": [
        "# MNIST dataset\n",
        "train_dataset = torchvision.datasets.MNIST(root='../../data',train=True,transform=transforms.ToTensor(),download=True)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=True)\n",
        "test_dataset = torchvision.datasets.MNIST(root='../../data',train=False,transform=transforms.ToTensor())\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,batch_size=batch_size,shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "52oUhHcnpZxw"
      },
      "outputs": [],
      "source": [
        "# dir to store reconstructed and sampled images\n",
        "sample_dir = 'results'\n",
        "if not os.path.exists(sample_dir):\n",
        "  os.makedirs(sample_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "rk8VBduWpZ0G"
      },
      "outputs": [],
      "source": [
        "# VAE model\n",
        "class VAE(nn.Module):\n",
        "  def __init__(self):\n",
        "    ## since we want to inherit everything from nn.Module\n",
        "    super(VAE,self).__init__()\n",
        "    self.fc1 = nn.Linear(image_size,hidden_dim)\n",
        "    self.fc2_mean = nn.Linear(hidden_dim,latent_dim)\n",
        "    self.fc2_logvar = nn.Linear(hidden_dim,latent_dim)\n",
        "    self.fc3 = nn.Linear(latent_dim,hidden_dim)\n",
        "    self.fc4 = nn.Linear(hidden_dim,image_size)\n",
        "\n",
        "  def encode(self,x):\n",
        "    h = F.relu(self.fc1(x))\n",
        "    mu = self.fc2_mean(h)\n",
        "    log_var = self.fc2_logvar(h)\n",
        "    return mu, log_var\n",
        "\n",
        "  def reparameterize(self, mu, logvar):\n",
        "    # for numerical stability\n",
        "    std = torch.exp(logvar/2)\n",
        "    # element-wise multiplication so same shape\n",
        "    eps = torch.randn_like(std)\n",
        "    return mu + eps * std\n",
        "\n",
        "  def decode(self, z):\n",
        "    h = F.relu(self.fc3(z))\n",
        "    # i/p and o/p values should be between 0 and 1\n",
        "    out = torch.sigmoid(self.fc4(h))\n",
        "    return out\n",
        "\n",
        "  def forward(self,x):\n",
        "    # x: (batch_size,1,28,28) -> (batch_size,784)\n",
        "    mu, logvar = self.encode(x.view(-1, image_size))\n",
        "    z = self.reparameterize(mu, logvar)\n",
        "    reconstructed = self.decode(z)\n",
        "    return reconstructed, mu, logvar\n",
        "\n",
        "# Define model and optimizer\n",
        "model = VAE().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "mCGYN43OpZ2U"
      },
      "outputs": [],
      "source": [
        "# Define loss = bce + kld\n",
        "def loss_function(reconstructed_image, original_image, mu, logvar):\n",
        "  bce = F.binary_cross_entropy(reconstructed_image, original_image.view(-1,784), reduction='sum')\n",
        "  kld = 0.5 * torch.sum(logvar.exp() + mu.pow(2) - 1 - logvar)\n",
        "  return bce + kld\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "  model.train()\n",
        "  train_loss = 0\n",
        "  for i, (images, _) in enumerate(train_loader):\n",
        "    images = images.to(device)\n",
        "    reconstructed, mu, logvar = model(images)\n",
        "    loss = loss_function(reconstructed, images, mu, logvar)\n",
        "    optimizer.zero_grad() # don't want to accumulate\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    if i%100 == 0:\n",
        "      print(\"Train Epoch {} [Batch {}/{}]\\tLoss: {:.3f}\".format(epoch,i,len(train_loader),loss.item()/len(images)))\n",
        "  print(\"=======> Epoch {}, Average Loss: {:.3f}\".format(epoch, train_loss/len(train_loader.dataset)))\n",
        "\n",
        "def test(epoch):\n",
        "  model.eval()\n",
        "  test_loss = 0\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, (images, _) in enumerate(test_loader):\n",
        "      images = images.to(device)\n",
        "      reconstructed, mu, logvar = model(images)\n",
        "      loss = loss_function(reconstructed, images, mu, logvar)\n",
        "      test_loss += loss.item()\n",
        "      if batch_idx == 0: # at each epoch in the beginning of training\n",
        "        comparison = torch.cat([images[:5], reconstructed.view(batch_size,1,28,28)[:5]])\n",
        "        save_image(comparison.cpu(),'results/reconstruction_' + str(epoch) + '.png',nrow = 5)\n",
        "\n",
        "  print(\"=======> Average Tess Loss: {:.3f}\".format(test_loss/len(test_loader.dataset)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpK_hthDpZ6u",
        "outputId": "4e774885-18a5-47e2-e111-45f6465444c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch 1 [Batch 0/469]\tLoss: 546.414\n",
            "Train Epoch 1 [Batch 100/469]\tLoss: 182.216\n",
            "Train Epoch 1 [Batch 200/469]\tLoss: 154.341\n",
            "Train Epoch 1 [Batch 300/469]\tLoss: 142.859\n",
            "Train Epoch 1 [Batch 400/469]\tLoss: 130.953\n",
            "=======> Epoch 1, Average Loss: 164.498\n",
            "=======> Average Tess Loss: 128.839\n",
            "Train Epoch 2 [Batch 0/469]\tLoss: 127.327\n",
            "Train Epoch 2 [Batch 100/469]\tLoss: 125.744\n",
            "Train Epoch 2 [Batch 200/469]\tLoss: 124.687\n",
            "Train Epoch 2 [Batch 300/469]\tLoss: 119.464\n",
            "Train Epoch 2 [Batch 400/469]\tLoss: 115.358\n",
            "=======> Epoch 2, Average Loss: 122.304\n",
            "=======> Average Tess Loss: 116.424\n",
            "Train Epoch 3 [Batch 0/469]\tLoss: 117.581\n",
            "Train Epoch 3 [Batch 100/469]\tLoss: 115.210\n",
            "Train Epoch 3 [Batch 200/469]\tLoss: 118.075\n",
            "Train Epoch 3 [Batch 300/469]\tLoss: 118.622\n",
            "Train Epoch 3 [Batch 400/469]\tLoss: 113.291\n",
            "=======> Epoch 3, Average Loss: 114.816\n",
            "=======> Average Tess Loss: 112.099\n",
            "Train Epoch 4 [Batch 0/469]\tLoss: 110.082\n",
            "Train Epoch 4 [Batch 100/469]\tLoss: 115.408\n",
            "Train Epoch 4 [Batch 200/469]\tLoss: 115.385\n",
            "Train Epoch 4 [Batch 300/469]\tLoss: 112.523\n",
            "Train Epoch 4 [Batch 400/469]\tLoss: 112.028\n",
            "=======> Epoch 4, Average Loss: 111.640\n",
            "=======> Average Tess Loss: 109.604\n",
            "Train Epoch 5 [Batch 0/469]\tLoss: 109.684\n",
            "Train Epoch 5 [Batch 100/469]\tLoss: 115.846\n",
            "Train Epoch 5 [Batch 200/469]\tLoss: 111.845\n",
            "Train Epoch 5 [Batch 300/469]\tLoss: 108.830\n",
            "Train Epoch 5 [Batch 400/469]\tLoss: 107.776\n",
            "=======> Epoch 5, Average Loss: 109.863\n",
            "=======> Average Tess Loss: 108.338\n",
            "Train Epoch 6 [Batch 0/469]\tLoss: 111.936\n",
            "Train Epoch 6 [Batch 100/469]\tLoss: 110.309\n",
            "Train Epoch 6 [Batch 200/469]\tLoss: 107.797\n",
            "Train Epoch 6 [Batch 300/469]\tLoss: 110.684\n",
            "Train Epoch 6 [Batch 400/469]\tLoss: 105.845\n",
            "=======> Epoch 6, Average Loss: 108.652\n",
            "=======> Average Tess Loss: 107.496\n",
            "Train Epoch 7 [Batch 0/469]\tLoss: 106.856\n",
            "Train Epoch 7 [Batch 100/469]\tLoss: 109.659\n",
            "Train Epoch 7 [Batch 200/469]\tLoss: 106.821\n",
            "Train Epoch 7 [Batch 300/469]\tLoss: 109.645\n",
            "Train Epoch 7 [Batch 400/469]\tLoss: 108.349\n",
            "=======> Epoch 7, Average Loss: 107.768\n",
            "=======> Average Tess Loss: 106.818\n",
            "Train Epoch 8 [Batch 0/469]\tLoss: 108.844\n",
            "Train Epoch 8 [Batch 100/469]\tLoss: 104.453\n",
            "Train Epoch 8 [Batch 200/469]\tLoss: 104.545\n",
            "Train Epoch 8 [Batch 300/469]\tLoss: 102.604\n",
            "Train Epoch 8 [Batch 400/469]\tLoss: 110.120\n",
            "=======> Epoch 8, Average Loss: 107.078\n",
            "=======> Average Tess Loss: 106.282\n",
            "Train Epoch 9 [Batch 0/469]\tLoss: 104.915\n",
            "Train Epoch 9 [Batch 100/469]\tLoss: 106.192\n",
            "Train Epoch 9 [Batch 200/469]\tLoss: 106.174\n",
            "Train Epoch 9 [Batch 300/469]\tLoss: 106.786\n",
            "Train Epoch 9 [Batch 400/469]\tLoss: 108.106\n",
            "=======> Epoch 9, Average Loss: 106.549\n",
            "=======> Average Tess Loss: 105.945\n",
            "Train Epoch 10 [Batch 0/469]\tLoss: 105.129\n",
            "Train Epoch 10 [Batch 100/469]\tLoss: 105.678\n",
            "Train Epoch 10 [Batch 200/469]\tLoss: 105.634\n",
            "Train Epoch 10 [Batch 300/469]\tLoss: 105.103\n",
            "Train Epoch 10 [Batch 400/469]\tLoss: 106.528\n",
            "=======> Epoch 10, Average Loss: 106.190\n",
            "=======> Average Tess Loss: 105.474\n",
            "Train Epoch 11 [Batch 0/469]\tLoss: 107.518\n",
            "Train Epoch 11 [Batch 100/469]\tLoss: 105.661\n",
            "Train Epoch 11 [Batch 200/469]\tLoss: 112.191\n",
            "Train Epoch 11 [Batch 300/469]\tLoss: 104.827\n",
            "Train Epoch 11 [Batch 400/469]\tLoss: 104.496\n",
            "=======> Epoch 11, Average Loss: 105.723\n",
            "=======> Average Tess Loss: 105.219\n",
            "Train Epoch 12 [Batch 0/469]\tLoss: 102.749\n",
            "Train Epoch 12 [Batch 100/469]\tLoss: 108.500\n",
            "Train Epoch 12 [Batch 200/469]\tLoss: 104.531\n",
            "Train Epoch 12 [Batch 300/469]\tLoss: 107.519\n",
            "Train Epoch 12 [Batch 400/469]\tLoss: 102.462\n",
            "=======> Epoch 12, Average Loss: 105.456\n",
            "=======> Average Tess Loss: 104.878\n",
            "Train Epoch 13 [Batch 0/469]\tLoss: 107.097\n",
            "Train Epoch 13 [Batch 100/469]\tLoss: 107.231\n",
            "Train Epoch 13 [Batch 200/469]\tLoss: 108.621\n",
            "Train Epoch 13 [Batch 300/469]\tLoss: 106.483\n",
            "Train Epoch 13 [Batch 400/469]\tLoss: 101.448\n",
            "=======> Epoch 13, Average Loss: 105.132\n",
            "=======> Average Tess Loss: 104.634\n",
            "Train Epoch 14 [Batch 0/469]\tLoss: 104.626\n",
            "Train Epoch 14 [Batch 100/469]\tLoss: 100.539\n",
            "Train Epoch 14 [Batch 200/469]\tLoss: 105.747\n",
            "Train Epoch 14 [Batch 300/469]\tLoss: 106.626\n",
            "Train Epoch 14 [Batch 400/469]\tLoss: 104.917\n",
            "=======> Epoch 14, Average Loss: 104.914\n",
            "=======> Average Tess Loss: 104.327\n",
            "Train Epoch 15 [Batch 0/469]\tLoss: 105.733\n",
            "Train Epoch 15 [Batch 100/469]\tLoss: 103.102\n",
            "Train Epoch 15 [Batch 200/469]\tLoss: 101.908\n",
            "Train Epoch 15 [Batch 300/469]\tLoss: 102.164\n",
            "Train Epoch 15 [Batch 400/469]\tLoss: 109.063\n",
            "=======> Epoch 15, Average Loss: 104.673\n",
            "=======> Average Tess Loss: 104.431\n",
            "Train Epoch 16 [Batch 0/469]\tLoss: 108.638\n",
            "Train Epoch 16 [Batch 100/469]\tLoss: 105.192\n",
            "Train Epoch 16 [Batch 200/469]\tLoss: 101.043\n",
            "Train Epoch 16 [Batch 300/469]\tLoss: 103.558\n",
            "Train Epoch 16 [Batch 400/469]\tLoss: 102.996\n",
            "=======> Epoch 16, Average Loss: 104.496\n",
            "=======> Average Tess Loss: 104.156\n",
            "Train Epoch 17 [Batch 0/469]\tLoss: 104.509\n",
            "Train Epoch 17 [Batch 100/469]\tLoss: 103.569\n",
            "Train Epoch 17 [Batch 200/469]\tLoss: 104.940\n",
            "Train Epoch 17 [Batch 300/469]\tLoss: 106.882\n",
            "Train Epoch 17 [Batch 400/469]\tLoss: 100.778\n",
            "=======> Epoch 17, Average Loss: 104.257\n",
            "=======> Average Tess Loss: 104.297\n",
            "Train Epoch 18 [Batch 0/469]\tLoss: 104.681\n",
            "Train Epoch 18 [Batch 100/469]\tLoss: 104.708\n",
            "Train Epoch 18 [Batch 200/469]\tLoss: 106.005\n",
            "Train Epoch 18 [Batch 300/469]\tLoss: 103.815\n",
            "Train Epoch 18 [Batch 400/469]\tLoss: 107.824\n",
            "=======> Epoch 18, Average Loss: 104.086\n",
            "=======> Average Tess Loss: 103.821\n",
            "Train Epoch 19 [Batch 0/469]\tLoss: 103.752\n",
            "Train Epoch 19 [Batch 100/469]\tLoss: 100.465\n",
            "Train Epoch 19 [Batch 200/469]\tLoss: 107.010\n",
            "Train Epoch 19 [Batch 300/469]\tLoss: 102.340\n",
            "Train Epoch 19 [Batch 400/469]\tLoss: 107.406\n",
            "=======> Epoch 19, Average Loss: 103.948\n",
            "=======> Average Tess Loss: 103.536\n",
            "Train Epoch 20 [Batch 0/469]\tLoss: 101.580\n",
            "Train Epoch 20 [Batch 100/469]\tLoss: 106.278\n",
            "Train Epoch 20 [Batch 200/469]\tLoss: 105.141\n",
            "Train Epoch 20 [Batch 300/469]\tLoss: 102.194\n",
            "Train Epoch 20 [Batch 400/469]\tLoss: 104.118\n",
            "=======> Epoch 20, Average Loss: 103.778\n",
            "=======> Average Tess Loss: 103.563\n",
            "Train Epoch 21 [Batch 0/469]\tLoss: 105.931\n",
            "Train Epoch 21 [Batch 100/469]\tLoss: 104.834\n",
            "Train Epoch 21 [Batch 200/469]\tLoss: 102.817\n",
            "Train Epoch 21 [Batch 300/469]\tLoss: 105.098\n",
            "Train Epoch 21 [Batch 400/469]\tLoss: 101.097\n",
            "=======> Epoch 21, Average Loss: 103.638\n",
            "=======> Average Tess Loss: 103.174\n",
            "Train Epoch 22 [Batch 0/469]\tLoss: 102.885\n",
            "Train Epoch 22 [Batch 100/469]\tLoss: 100.976\n",
            "Train Epoch 22 [Batch 200/469]\tLoss: 104.174\n",
            "Train Epoch 22 [Batch 300/469]\tLoss: 101.173\n",
            "Train Epoch 22 [Batch 400/469]\tLoss: 101.209\n",
            "=======> Epoch 22, Average Loss: 103.526\n",
            "=======> Average Tess Loss: 103.238\n",
            "Train Epoch 23 [Batch 0/469]\tLoss: 100.796\n",
            "Train Epoch 23 [Batch 100/469]\tLoss: 101.014\n",
            "Train Epoch 23 [Batch 200/469]\tLoss: 101.750\n",
            "Train Epoch 23 [Batch 300/469]\tLoss: 102.440\n",
            "Train Epoch 23 [Batch 400/469]\tLoss: 103.057\n",
            "=======> Epoch 23, Average Loss: 103.454\n",
            "=======> Average Tess Loss: 103.095\n",
            "Train Epoch 24 [Batch 0/469]\tLoss: 104.918\n",
            "Train Epoch 24 [Batch 100/469]\tLoss: 100.854\n",
            "Train Epoch 24 [Batch 200/469]\tLoss: 108.752\n",
            "Train Epoch 24 [Batch 300/469]\tLoss: 100.989\n",
            "Train Epoch 24 [Batch 400/469]\tLoss: 107.166\n",
            "=======> Epoch 24, Average Loss: 103.309\n",
            "=======> Average Tess Loss: 103.142\n",
            "Train Epoch 25 [Batch 0/469]\tLoss: 102.167\n",
            "Train Epoch 25 [Batch 100/469]\tLoss: 98.392\n",
            "Train Epoch 25 [Batch 200/469]\tLoss: 104.942\n",
            "Train Epoch 25 [Batch 300/469]\tLoss: 104.787\n",
            "Train Epoch 25 [Batch 400/469]\tLoss: 101.218\n",
            "=======> Epoch 25, Average Loss: 103.184\n",
            "=======> Average Tess Loss: 103.056\n",
            "Train Epoch 26 [Batch 0/469]\tLoss: 105.825\n",
            "Train Epoch 26 [Batch 100/469]\tLoss: 101.927\n",
            "Train Epoch 26 [Batch 200/469]\tLoss: 100.082\n",
            "Train Epoch 26 [Batch 300/469]\tLoss: 96.892\n",
            "Train Epoch 26 [Batch 400/469]\tLoss: 99.098\n",
            "=======> Epoch 26, Average Loss: 103.127\n",
            "=======> Average Tess Loss: 103.067\n",
            "Train Epoch 27 [Batch 0/469]\tLoss: 101.364\n",
            "Train Epoch 27 [Batch 100/469]\tLoss: 100.246\n",
            "Train Epoch 27 [Batch 200/469]\tLoss: 100.516\n",
            "Train Epoch 27 [Batch 300/469]\tLoss: 100.339\n",
            "Train Epoch 27 [Batch 400/469]\tLoss: 102.264\n",
            "=======> Epoch 27, Average Loss: 103.000\n",
            "=======> Average Tess Loss: 102.869\n",
            "Train Epoch 28 [Batch 0/469]\tLoss: 100.821\n",
            "Train Epoch 28 [Batch 100/469]\tLoss: 99.855\n",
            "Train Epoch 28 [Batch 200/469]\tLoss: 106.473\n",
            "Train Epoch 28 [Batch 300/469]\tLoss: 101.536\n",
            "Train Epoch 28 [Batch 400/469]\tLoss: 97.609\n",
            "=======> Epoch 28, Average Loss: 102.884\n",
            "=======> Average Tess Loss: 102.876\n",
            "Train Epoch 29 [Batch 0/469]\tLoss: 106.711\n",
            "Train Epoch 29 [Batch 100/469]\tLoss: 103.662\n",
            "Train Epoch 29 [Batch 200/469]\tLoss: 103.703\n",
            "Train Epoch 29 [Batch 300/469]\tLoss: 102.280\n",
            "Train Epoch 29 [Batch 400/469]\tLoss: 99.302\n",
            "=======> Epoch 29, Average Loss: 102.840\n",
            "=======> Average Tess Loss: 102.788\n",
            "Train Epoch 30 [Batch 0/469]\tLoss: 103.602\n",
            "Train Epoch 30 [Batch 100/469]\tLoss: 101.468\n",
            "Train Epoch 30 [Batch 200/469]\tLoss: 103.229\n",
            "Train Epoch 30 [Batch 300/469]\tLoss: 97.682\n",
            "Train Epoch 30 [Batch 400/469]\tLoss: 102.368\n",
            "=======> Epoch 30, Average Loss: 102.753\n",
            "=======> Average Tess Loss: 102.776\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(1,epochs+1):\n",
        "  train(epoch)\n",
        "  test(epoch)\n",
        "  with torch.no_grad():\n",
        "    # get rid of the encoder and sample some values from the gaussian distribution and feed it to the decoder to generate new samples\n",
        "    sample = torch.randn(64,20).to(device)\n",
        "    generated = model.decode(sample).cpu()\n",
        "    save_image(generated.view(64,1,28,28),'results/sample_' + str(epoch) + '.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgavBWQmpZ9W",
        "outputId": "369f0b22-9c83-47cb-d2b6-18e2459395b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2N3l9iYCpZ_6",
        "outputId": "6ce77969-55f8-4d59-a7b9-909d797eb012"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: results/ (stored 0%)\n",
            "  adding: results/reconstruction_7.png (deflated 1%)\n",
            "  adding: results/reconstruction_3.png (deflated 2%)\n",
            "  adding: results/reconstruction_2.png (deflated 3%)\n",
            "  adding: results/reconstruction_18.png (deflated 2%)\n",
            "  adding: results/sample_1.png (deflated 2%)\n",
            "  adding: results/reconstruction_23.png (deflated 3%)\n",
            "  adding: results/reconstruction_15.png (deflated 2%)\n",
            "  adding: results/reconstruction_26.png (deflated 2%)\n",
            "  adding: results/sample_22.png (deflated 4%)\n",
            "  adding: results/sample_18.png (deflated 4%)\n",
            "  adding: results/reconstruction_14.png (deflated 2%)\n",
            "  adding: results/sample_25.png (deflated 4%)\n",
            "  adding: results/sample_3.png (deflated 3%)\n",
            "  adding: results/reconstruction_20.png (deflated 2%)\n",
            "  adding: results/sample_13.png (deflated 4%)\n",
            "  adding: results/reconstruction_1.png (deflated 1%)\n",
            "  adding: results/sample_20.png (deflated 4%)\n",
            "  adding: results/sample_26.png (deflated 4%)\n",
            "  adding: results/sample_8.png (deflated 4%)\n",
            "  adding: results/reconstruction_11.png (deflated 2%)\n",
            "  adding: results/sample_4.png (deflated 3%)\n",
            "  adding: results/sample_28.png (deflated 5%)\n",
            "  adding: results/sample_24.png (deflated 4%)\n",
            "  adding: results/sample_6.png (deflated 4%)\n",
            "  adding: results/reconstruction_25.png (deflated 2%)\n",
            "  adding: results/reconstruction_29.png (deflated 2%)\n",
            "  adding: results/reconstruction_24.png (deflated 1%)\n",
            "  adding: results/sample_9.png (deflated 4%)\n",
            "  adding: results/reconstruction_21.png (deflated 2%)\n",
            "  adding: results/reconstruction_12.png (deflated 2%)\n",
            "  adding: results/sample_10.png (deflated 4%)\n",
            "  adding: results/sample_29.png (deflated 4%)\n",
            "  adding: results/sample_2.png (deflated 3%)\n",
            "  adding: results/sample_12.png (deflated 4%)\n",
            "  adding: results/sample_14.png (deflated 4%)\n",
            "  adding: results/reconstruction_17.png (deflated 2%)\n",
            "  adding: results/sample_16.png (deflated 4%)\n",
            "  adding: results/reconstruction_16.png (deflated 2%)\n",
            "  adding: results/reconstruction_4.png (deflated 2%)\n",
            "  adding: results/sample_7.png (deflated 4%)\n",
            "  adding: results/sample_27.png (deflated 4%)\n",
            "  adding: results/sample_23.png (deflated 4%)\n",
            "  adding: results/reconstruction_5.png (deflated 2%)\n",
            "  adding: results/reconstruction_9.png (deflated 2%)\n",
            "  adding: results/sample_15.png (deflated 4%)\n",
            "  adding: results/reconstruction_27.png (deflated 2%)\n",
            "  adding: results/sample_21.png (deflated 4%)\n",
            "  adding: results/sample_19.png (deflated 4%)\n",
            "  adding: results/reconstruction_22.png (deflated 2%)\n",
            "  adding: results/sample_17.png (deflated 4%)\n",
            "  adding: results/reconstruction_6.png (deflated 1%)\n",
            "  adding: results/reconstruction_8.png (deflated 2%)\n",
            "  adding: results/sample_30.png (deflated 4%)\n",
            "  adding: results/reconstruction_10.png (deflated 2%)\n",
            "  adding: results/reconstruction_19.png (deflated 1%)\n",
            "  adding: results/reconstruction_13.png (deflated 2%)\n",
            "  adding: results/sample_11.png (deflated 4%)\n",
            "  adding: results/reconstruction_30.png (deflated 2%)\n",
            "  adding: results/sample_5.png (deflated 4%)\n",
            "  adding: results/reconstruction_28.png (deflated 2%)\n"
          ]
        }
      ],
      "source": [
        "!zip -r results.zip results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "77-M3ly5paCj"
      },
      "outputs": [],
      "source": [
        "!mv results.zip /content/drive/MyDrive/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "NRfgt_ADpaEp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "q-yp6_4UpaHP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "y_L0u5gEpaNM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "eIKMsjkBpaPj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "opisfKmWpaVC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "KTttCxv7paXR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "MrZSgMTepaZ2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "-uvf3ibLpacT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "fvd0UAe4paeq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "TpR3wFQHpahH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "iXlx9_aepajg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "XusMC_M5pal2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "OKy63xzapaoD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "skeENQeJpaqr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "kAcd3rcypatB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "h6SbFkmqpavd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
